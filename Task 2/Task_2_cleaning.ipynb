{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeandclean(description):\n",
    "    # to be applied also to the query\n",
    "    \n",
    "    low_descr = str.lower(description)\n",
    "    \n",
    "    # We tokenize the description and remove puncuation\n",
    "    tok_descr = tokenizer.tokenize(low_descr)\n",
    "    # Alternative way: first tokenize then remove punctuation\n",
    "    # tok_descr = nltk.word_tokenize(low_descr)\n",
    "    # nltk.download(\"punkt\")\n",
    "    # no_pun_descr = [word for word in tok_descr if word.isalnum()]\n",
    "    \n",
    "    # We remove stopwords from tokenized description\n",
    "    no_stop_descr = [word for word in tok_descr if not word in stopwords.words()]\n",
    "    \n",
    "    # We carry out stemming\n",
    "    stem_descr = [ps.stem(i) for i in no_stop_descr]\n",
    "    \n",
    "    # We remove isolated characters\n",
    "    final_descr = [i for i in stem_descr if len(i) > 1]\n",
    "    \n",
    "    return final_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Description'] = dataset.apply(lambda j: tokenizeandclean(j['Description']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['Description'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR CALCULATIN THE TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TfIdf(lenghtDictionary, lenghtTerm, numberOfElements):\n",
    "    TF = numberOfElements / #numer of total words in this single document.\n",
    "    IDF = lenghtDictionary / lenghtTerm\n",
    "    \n",
    "    return TF*IDF\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
