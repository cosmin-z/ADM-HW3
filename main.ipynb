{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import lxml\n",
    "import csv\n",
    "import psutil\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constante variaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_txt = \"https.txt\"\n",
    "folder = 'Task 1.2/dataset'\n",
    "folder2 = 'Task 1.3/anime_tsv'\n",
    "headers = {\n",
    "    'user-agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\",\n",
    "    'accept': \"image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8\",\n",
    "    'referer': \"https://myanimelist.net/\"\n",
    "} \n",
    "save_path = 'simplePath'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "reg = r'href=\\\"https://[\\w\\W]*\" id'\n",
    "htt = list()\n",
    "for i in range(400):\n",
    "    cnt = requests.get(\"https://myanimelist.net/topanime.php?limit=\"+str(i*50))\n",
    "    soup = BeautifulSoup(cnt.content, features=\"lxml\")\n",
    "    links = soup.find_all('h3')\n",
    "    for l in range(len(links)-3):\n",
    "        li = links[l].find('a')\n",
    "        m = re.findall(reg, str(li))\n",
    "        s = m[0][6:][:-4]+'\\n'\n",
    "        htt.append(s)\n",
    "f = open(\"urls_txt\", 'w')\n",
    "for i in htt:\n",
    "    f.write(i)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htmls_by_urls(urls_txt, folder):\n",
    "     # urls_txt: string 'https.txt' from previous task\n",
    "    # folder: string; eg '/Users/anton/Desktop/ADM/Homework3/html'\n",
    "    \n",
    "    with open(urls_txt, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # list of urls\n",
    "    list_txt = [line.strip() for line in lines]\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while i < len(list_txt):\n",
    "        url = list_txt[i]\n",
    "        # folder where we save html\n",
    "        al_folder = '{}/page_{}/{}.html'.format(folder, i//50 +1, i+1)\n",
    "        # download html\n",
    "        html = requests.get(url, headers)\n",
    "        if(html.status_code != 200) : \n",
    "            time.sleep(120)\n",
    "            print('error', html.status_code)\n",
    "        else:\n",
    "            i += 1\n",
    "            with open(al_folder, 'w', encoding='utf-8') as g:\n",
    "                g.write(html.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 403\n",
      "error 403\n",
      "error 403\n",
      "error 403\n"
     ]
    }
   ],
   "source": [
    "htmls_by_urls(urls_txt, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19053\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir(folder)\n",
    "alarr = list()\n",
    "for y in arr:\n",
    "    if \"rar\" in y:\n",
    "        continue\n",
    "    fiarr = os.listdir(folder+'/'+y)\n",
    "    if '.ipynb_checkpoints' in fiarr:\n",
    "        fiarr.remove('.ipynb_checkpoints')\n",
    "    for i in range(len(fiarr)):\n",
    "        fiarr[i] = y+'/'+fiarr[i]\n",
    "    alarr.extend(fiarr)\n",
    "print(len(alarr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1.2/dataset/page_107/5350.html 499\n",
      "Task 1.2/dataset/page_116/5800.html 999\n",
      "Task 1.2/dataset/page_125/6250.html 1499\n",
      "Task 1.2/dataset/page_134/6700.html 1999\n",
      "Task 1.2/dataset/page_143/7150.html 2499\n",
      "Task 1.2/dataset/page_152/7600.html 2999\n",
      "Task 1.2/dataset/page_161/8050.html 3499\n",
      "Task 1.2/dataset/page_170/8500.html 3999\n",
      "Task 1.2/dataset/page_18/900.html 4499\n",
      "Task 1.2/dataset/page_189/9450.html 4999\n",
      "Task 1.2/dataset/page_198/9900.html 5499\n",
      "Task 1.2/dataset/page_206/10300.html 5999\n",
      "Task 1.2/dataset/page_215/10750.html 6499\n",
      "Task 1.2/dataset/page_224/11200.html 6999\n",
      "Task 1.2/dataset/page_233/11650.html 7499\n",
      "Task 1.2/dataset/page_242/12100.html 7999\n",
      "Task 1.2/dataset/page_251/12550.html 8499\n",
      "Task 1.2/dataset/page_260/13000.html 8999\n",
      "Task 1.2/dataset/page_27/1350.html 9499\n",
      "Task 1.2/dataset/page_279/13950.html 9999\n",
      "Task 1.2/dataset/page_288/14400.html 10499\n",
      "Task 1.2/dataset/page_297/14850.html 10999\n",
      "Task 1.2/dataset/page_305/15250.html 11499\n",
      "Task 1.2/dataset/page_314/15700.html 11999\n",
      "Task 1.2/dataset/page_323/16150.html 12499\n",
      "Task 1.2/dataset/page_332/16600.html 12999\n",
      "Task 1.2/dataset/page_341/17050.html 13499\n",
      "Task 1.2/dataset/page_350/17500.html 13999\n",
      "Task 1.2/dataset/page_36/1800.html 14499\n",
      "Task 1.2/dataset/page_369/18450.html 14999\n",
      "Task 1.2/dataset/page_378/18900.html 15499\n",
      "Task 1.2/dataset/page_43/2147.html 15999\n",
      "Task 1.2/dataset/page_52/2597.html 16499\n",
      "Task 1.2/dataset/page_61/3047.html 16999\n",
      "Task 1.2/dataset/page_70/3497.html 17499\n",
      "Task 1.2/dataset/page_8/397.html 17999\n",
      "Task 1.2/dataset/page_89/4447.html 18499\n"
     ]
    }
   ],
   "source": [
    "reg1 = r'\\n[\\w\\W]*to'\n",
    "reg2 = r'to[\\w\\W]*\\n'\n",
    "trfile = 'file.csv'\n",
    "animeTitle = []\n",
    "animeTypes = []\n",
    "animeNumEpisode = []\n",
    "releaseDate = []\n",
    "endDate = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = [] \n",
    "df = pd.DataFrame(\n",
    "[animeTitle, animeTypes, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff], \n",
    "index=['Title', 'Type', 'Episodes','Release date', 'End date', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff']\n",
    ").T\n",
    "bs = 9\n",
    "for fiName in range(len(alarr)):\n",
    "    # anime titles\n",
    "    path = folder+'/'+alarr[fiName]\n",
    "    #print(path)\n",
    "    file = codecs.open(path, \"r\", \"utf-8\")\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    animeTitle.append(soup.find_all('strong')[0].contents[0])\n",
    "    \n",
    "    # left sied of the html------------------------------------------------------------\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"spaceit_pad\"})\n",
    "    for div in divs:\n",
    "        spans = div.find_all(\"span\")\n",
    "        for span in spans:\n",
    "            # anime types\n",
    "            if span.contents[0] == 'Type:':\n",
    "                tr = div.find_all('a')\n",
    "                if len(tr) ==0:\n",
    "                    animeTypes.append(div.contents[2][2:])\n",
    "                else:\n",
    "                    animeTypes.append(div.find_all('a')[0].contents[0])\n",
    "            # anime number of episodes\n",
    "            if span.contents[0] == 'Episodes:':\n",
    "                try :\n",
    "                    animeNumEpisode.append(int(div.contents[2]))\n",
    "                except:\n",
    "                    animeNumEpisode.append(0)\n",
    "            # anime release and end dates\n",
    "            if span.contents[0] == 'Aired:':\n",
    "                if 'Not available' in div.contents[2]:\n",
    "                    releaseDate.append('?')\n",
    "                    endDate.append('?')\n",
    "                elif 'to' in div.contents[2]:                    \n",
    "                    if \"?\" in re.findall(reg2, str(div.contents[2]))[0][3:-1]:\n",
    "                        endDate.append(\"?\")\n",
    "                    else:\n",
    "                        endDate.append(pd.to_datetime(re.findall(reg2, str(div.contents[2]))[0][3:-1]))\n",
    "                    if \"?\" in re.findall(reg1, str(div.contents[2]))[0][3:-1]:\n",
    "                        releaseDate.append(\"?\")\n",
    "                    else:\n",
    "                        releaseDate.append(pd.to_datetime(re.findall(reg1, str(div.contents[2]))[0][2:-3]))\n",
    "                else:\n",
    "                    releaseDate.append(pd.to_datetime(div.contents[2][2:-2]))\n",
    "                    endDate.append('-')\n",
    "                    \n",
    "    # middle side of the html------------------------------------------------------------\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "    for div in divs:\n",
    "        # anime number of members\n",
    "        members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "        animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "        # anime score\n",
    "        score = div.find_all(\"div\", {\"class\": \"score-label score-\"+str(bs)})\n",
    "        while len(score) == 0:\n",
    "            bs -= 1\n",
    "            score = div.find_all(\"div\", {\"class\": \"score-label score-\"+str(bs)})\n",
    "            if bs == -1:\n",
    "                bs = 10\n",
    "                score = div.find_all(\"div\", {\"class\": \"score-label score-na\"})\n",
    "        try:        \n",
    "            animeScore.append(float(score[0].contents[0]))\n",
    "            # anime number of users\n",
    "            users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "            animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "        except:\n",
    "            animeScore.append(None)\n",
    "            animeUsers.append(0)\n",
    "\n",
    "        # anime rank\n",
    "        rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "        try:\n",
    "            animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "        except:\n",
    "            animeRank.append(-1)\n",
    "        # anime popularity\n",
    "        popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "        animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "        \n",
    "    # anime description\n",
    "    description = soup.find_all(\"p\", {\"itemprop\": \"description\"})\n",
    "    for br in description[0].find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    animeDescription.append(description[0].contents)\n",
    "    \n",
    "    # related animes \n",
    "    related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    x = []\n",
    "    y = []\n",
    "    for tr in related:\n",
    "        td = tr.find_all(\"td\")\n",
    "        for i in range(0, len(td), 2):\n",
    "            x.append(td[i].contents[0])\n",
    "            try:\n",
    "                t = td[i+1].find_all(\"a\")\n",
    "                y.append(t[0].contents[0])\n",
    "            except:\n",
    "                y.append('NA')\n",
    "            \n",
    "        animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "        if(len(animeRelated)<len(animeDescription)):\n",
    "            animeRelated.append(\"None\")\n",
    "    try:    \n",
    "        # anime characters and voices\n",
    "        characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "        x = []\n",
    "        y = []\n",
    "        voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "        for i in chars:\n",
    "            x.append(i.contents[0].contents[0])\n",
    "        for i in voices:\n",
    "            y.append(i.contents[1].contents[0])\n",
    "        animeCharacters.append(x)\n",
    "        animeVoices.append(y)\n",
    "    except:\n",
    "        animeCharacters.append(\"NA\")\n",
    "        animeVoices.append(\"NA\")\n",
    "    \n",
    "    # anime staff\n",
    "    try:\n",
    "        staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        staff = staff[1].find_all(\"td\")\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(1, len(staff), 2):\n",
    "            x.append(staff[i].contents[1].contents[0])\n",
    "            y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "        animeStaff.append([list(i) for i in list(zip(x,y))])\n",
    "    except:\n",
    "        animeStaff.append(\"NA\")\n",
    "        \n",
    "        \n",
    "    # Following lines are made to save memory by storing the current info in a pd instead of the array    \n",
    "    if ((fiName+1)%500) == 0:\n",
    "        print(path,fiName)\n",
    "        dftr = pd.DataFrame(\n",
    "        [animeTitle.copy(), animeTypes.copy(), animeNumEpisode.copy(), releaseDate.copy(), endDate.copy(), animeNumMembers.copy(), animeScore.copy(), animeUsers.copy(), animeRank.copy(), animePopularity.copy(), animeDescription.copy(), animeRelated.copy(), animeCharacters.copy(), animeVoices.copy(), animeStaff.copy()], \n",
    "        index=['Title', 'Type', 'Episodes','Release date', 'End date', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff']\n",
    "        ).T\n",
    "        df = pd.concat([df, dftr])\n",
    "        df.to_csv(trfile, index=False)\n",
    "        del df, dftr\n",
    "        animeTitle = []\n",
    "        animeTypes = []\n",
    "        animeNumEpisode = []\n",
    "        releaseDate = []\n",
    "        endDate = []\n",
    "        animeNumMembers = []\n",
    "        animeScore = []\n",
    "        animeUsers = []\n",
    "        animeRank = []\n",
    "        animePopularity = []\n",
    "        animeDescription = []\n",
    "        animeRelated = []\n",
    "        animeCharacters = []\n",
    "        animeVoices = []\n",
    "        animeStaff = [] \n",
    "        df = pd.read_csv(trfile)\n",
    "        os.remove(trfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr = pd.DataFrame(\n",
    "[animeTitle, animeTypes, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff], \n",
    "index=['Title', 'Type', 'Episodes','Release date', 'End date', 'Members', 'Score', 'Users', 'Rank', 'Popularity', 'Description', 'Related', 'Characters', 'Voices', 'Staff']\n",
    ").T\n",
    "df = pd.concat([df, dftr])\n",
    "df.to_csv(trfile, index=False)\n",
    "del df, dftr, animeTitle, animeTypes, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff\n",
    "df = pd.read_csv(trfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each row I created a tsv file with the value of each anime \n",
    "for i in range(len(df)):\n",
    "    with open(folder2+'/anime_'+str(i)+'.tsv', 'wt') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow([x for x in df.columns]) #the header row\n",
    "        tsv_writer.writerow(x for x in df.iloc[i]) #the value under each columns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(trfile, index=False)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
